{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawception Scraping Process\n",
    "Scraping drawception occurs in 3 stages. The first step is to collect the url's to the pages with the images on them. These pages are each referred to as games, and each game has between 6-12 image panels. After the game url's are collected the data from the panels can be scraped. It's important to allow some time to pass for the players of the game to react to the images. Any reactions made after the scrape will not be included in my data. The final step is to download the images to disk, which is done with a third function.\n",
    "\n",
    "For each step there is extra code to prevent collecting duplicate data and also allowing the data to be collected in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to be Collected\n",
    "\n",
    "The data specific to the image\n",
    "- pre_caption\n",
    "- post_caption\n",
    "- image_url\n",
    "- author\n",
    "- panel_number\n",
    "- LIKE\n",
    "- HAHA\n",
    "- WOW\n",
    "- LOVE\n",
    "- DUCK\n",
    "\n",
    "The data specific to the game (a series of 6-12 images)\n",
    "- game_url\n",
    "- player_num\n",
    "- game_duration\n",
    "- game_date\n",
    "- game_tags\n",
    "\n",
    "Extra features\n",
    "- REACT (the sum of LIKE HAHA WOW LOVE and DUCK)\n",
    "- image_path (local path to the image file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Game URL's\n",
    "\n",
    "The function scrapes from the drawception browse game page. There are only 100 pages availible to scrape at a time. It takes less than a week to refresh the entire list. The list is saved and loaded as a dataframe for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all pages in the browse games part of drawception up to specified number.\n",
    "# Thanks to: https://towardsdatascience.com/the-simplest-cleanest-method-for-tracking-a-for-loops-progress-and-expected-run-time-in-python-972675392b3\n",
    "def scrape_recent(stop_page):\n",
    "    \n",
    "    browse_games_url = 'https://drawception.com/browse/recent-games/'\n",
    "    game_list = []\n",
    "    \n",
    "    # Make sure I don't access a page that doesn't exist\n",
    "    if (stop_page > 100) | (stop_page <= 0):\n",
    "        stop_page = 100\n",
    "    \n",
    "    # look at all pages in reverse order, by going backwards if (when!) the list shifts in the middle of scraping \n",
    "    # we miss a game instead of getting a duplicate.\n",
    "    for i in range(stop_page,0,-1):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        res = requests.get(f'{browse_games_url}{i}/')\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        temp_list = [game.attrs['href'] for game in soup.find_all(attrs={'class':'thumbpanel'})]\n",
    "        game_list.extend(temp_list)\n",
    "        \n",
    "        print(f'Scraped page {i}')\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return game_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a Single Game\n",
    "\n",
    "This function takes a drawception game and records the game into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_game(game_url):\n",
    "    \n",
    "    # Collect page information\n",
    "    base_url = 'https://drawception.com'\n",
    "    res = requests.get(base_url+game_url)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    # Get panels variable and check that the game page has panels\n",
    "    # If no panels then something is up with the game so skip it. NSFW games do this.\n",
    "    panels = soup.find_all(attrs = {'class':'col-sm-12 col-md-4'})\n",
    "    if len(panels) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Escape if page doesn't load\n",
    "    if res.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    ### Header stuff that applies to all panels ###\n",
    "    #  game_url : player_num : game_duration: game_date : game_tags\n",
    "    \n",
    "    # html with tags like 'top game' color palette and other misc tags\n",
    "    top_padding = soup.find(attrs = {'class':'text-center add-padding-bottom2x add-padding-top'})\n",
    "    game_tags = []\n",
    "    if top_padding != None:\n",
    "        game_tags = list(set([s.text.strip() for s in top_padding.find_all(name='span')]))\n",
    "\n",
    "    # String with player count duration and game date\n",
    "    header_str = soup.find(name='p', attrs={'class':'lead text-muted add-margin-top2x'}).text\n",
    "    header_str = header_str.strip().split()\n",
    "\n",
    "    player_num = header_str[0]\n",
    "    game_date = ' '.join(header_str[6:9])\n",
    "    game_duration = ' '.join(header_str[11:13])\n",
    "    \n",
    "    \n",
    "    data_rows = []\n",
    "    for i in range(len(panels)):\n",
    "        if panels[i].find('img') != None:\n",
    "\n",
    "            this_row = {}\n",
    "            # Things to collect for this row:\n",
    "            #  panel-number : pre-caption : post-caption : image : author : smile : love : haha : wow : duck\n",
    "\n",
    "            # Check if this is the first panel then record the pre_caption\n",
    "            if i == 0:\n",
    "                this_row['pre_caption'] = 'draw_first'\n",
    "            else:\n",
    "                this_row['pre_caption'] = panels[i].find(name = 'img').attrs['title']\n",
    "\n",
    "            # Check if this is the last panel then record the post_caption\n",
    "            if i == (len(panels)-1):\n",
    "                this_row['post_caption'] = 'draw_last'\n",
    "            else:\n",
    "                this_row['post_caption'] = panels[i+1].find(name='p').text.strip()\n",
    "\n",
    "            # Image URL\n",
    "            this_row['image_url'] = panels[i].find('img').attrs['src']\n",
    "\n",
    "            # Author name (deleted or banned accounts appear as OooOOOoOo)\n",
    "            if panels[i].find(attrs = {'class':'panel-user'}).find('a') != None:\n",
    "                this_row['author'] = panels[i].find(attrs = {'class':'panel-user'}).find('a').text\n",
    "            else:\n",
    "                # Ghost name\n",
    "                this_row['author'] = panels[i].find(attrs = {'class':'panel-user'}).find('span').text\n",
    "                \n",
    "            # Panel number is simple to collect\n",
    "            this_row['panel_number'] = i+1\n",
    "\n",
    "            # Reactions could exist or not exist in a dictionary. Start by setting all of them to 0.\n",
    "            reaction_types = ['LIKE', 'HAHA', 'WOW', 'LOVE', 'DUCK']\n",
    "            react_data = json.loads(panels[i].find('reactions').attrs['reactions_data'])\n",
    "            for react in reaction_types:\n",
    "                this_row[react] = 0\n",
    "            # Now loop through the reaction dictionary and reset the non-zeros\n",
    "            for react_dict in react_data:\n",
    "                this_row[react_dict['id']] = react_dict['num']\n",
    "                \n",
    "            # Add in the universal game stuff here\n",
    "            # game_url : player_num : game_duration: game_date : game_tags\n",
    "            this_row['game_url'] = game_url\n",
    "            this_row['player_num'] = player_num\n",
    "            this_row['game_date'] = game_date\n",
    "            this_row['game_tags'] = game_tags\n",
    "\n",
    "            data_rows.append(this_row)\n",
    "            \n",
    "            # Sleep here, no matter how the function is called the sleep is included\n",
    "            time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a Batch of Games\n",
    "\n",
    "This function runs through a list of game URL's and downloads each game that isn't found in the dataframe already. This way I can control how much I'm downloading at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape list\n",
    "def batch_scrape(game_list, dataframe, batch_size):\n",
    "    # Count up to batch size like a while loop. Use a for loop to handle the edge case of finishing the list\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(game_list)):\n",
    "        if game_list[i] not in list(drawception['game_url']):\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            count += 1\n",
    "            \n",
    "            print(f'Scraped page {count}/{batch_size} - {game_list[i]}')\n",
    "            scrape_game(game_list[i])\n",
    "            dataframe = dataframe.append(scrape_game(game_list[i]))\n",
    "            \n",
    "            if count >= batch_size:\n",
    "                return dataframe\n",
    "    print('End of Game List')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Images\n",
    "\n",
    "This function looks at the dataframe of all the images and downloads any images that it can't find in the drawings directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to AlexG on stack exchange\n",
    "# https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know\n",
    "def image_scrape(dataframe, batch_size):\n",
    "\n",
    "    # Count up to batch size like a while loop. Use a for loop to handle the edge case of finishing the list\n",
    "    count = 0\n",
    "    \n",
    "    for _, row in dataframe.iterrows():\n",
    "        \n",
    "        # Get the file path from the url. My local data will mirror the url.\n",
    "        path_list = row['image_url'].split('/')\n",
    "        my_file = f'../{\"/\".join(path_list[3:6])}'\n",
    "        \n",
    "        # Check if the image is already downloaded\n",
    "        if not os.path.exists(my_file):\n",
    "        \n",
    "            # This code runs if the image needs scraping\n",
    "            clear_output(wait=True)\n",
    "            count += 1\n",
    "            print(f'Scraped image {count}/{batch_size} - {my_file}')\n",
    "\n",
    "            # Create directory if it doesn't exist\n",
    "            my_path = f'../{\"/\".join(path_list[3:5])}'\n",
    "            if not os.path.exists(my_path):\n",
    "                os.makedirs(my_path)\n",
    "\n",
    "            # Get the image from the web\n",
    "            page = requests.get(row['image_url'])\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Write image file\n",
    "            with open(my_file, 'wb') as f:\n",
    "                f.write(page.content)\n",
    "                \n",
    "            # Escape early so I can control how long this code runs\n",
    "            if count >= batch_size:\n",
    "                return\n",
    "                \n",
    "    print('End of DataFrame')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all into Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Get the game URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in saved games list and convert from dataframe back to list.\n",
    "# Alternatively you can just skip this step and just collect new stuff.\n",
    "games_df = pd.read_csv('../data/games_jan01.csv')\n",
    "games = games_df['0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape every game off the browse games section. max 100\n",
    "new_scrape = scrape_recent(5)\n",
    "len(new_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the lists together then gets rid of duplicates, possibly altering the order but that's fine.\n",
    "games.extend(new_scrape)\n",
    "games = list(set(games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a file, ### CHANGE THE FILENAME ###\n",
    "game_df = pd.DataFrame(new_scrape)\n",
    "game_df.to_csv('../data/games_jan17.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Get the panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously collected data.\n",
    "# Run first scrape to initialize the columns of the master DataFrame in the case it's the first time.\n",
    "# drawception = scrape_game(games[0])\n",
    "\n",
    "drawception = pd.read_csv('../data/drawception_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Game List\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40921, 14)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape games off the games list\n",
    "drawception = batch_scrape(games, drawception, 500)\n",
    "drawception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a file, ### CHANGE THE FILENAME ###\n",
    "drawception.to_csv('../data/drawception_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Get the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped image 1051/5000 - ../drawings/1040071/xYFZnpBFOw.png\n",
      "End of DataFrame\n"
     ]
    }
   ],
   "source": [
    "image_scrape(drawception, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
